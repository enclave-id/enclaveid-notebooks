{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf062c8-99b5-4e1e-81cd-af956311a4a4",
   "metadata": {},
   "source": [
    "# Enclave ID \n",
    "\n",
    "This notebook is solely for testing the inputs and outputs of a candidate workflow. Some components will either be completely changed or improved soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774b93e-24df-4a99-a915-bcaef910d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_client import Client\n",
    "import json\n",
    "import json_repair\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8cd51-859e-41a2-9727-c4f2f270d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_7B_CLIENT = Client(\"huggingface-projects/llama-2-7b-chat\")\n",
    "MISTRAL_7B_CLIENT = Client(\"hysts/mistral-7b\")\n",
    "\n",
    "\n",
    "def ask_llama2_7b(prompt, \n",
    "                  system_prompt=\"\",\n",
    "                  max_new_tokens=1024, \n",
    "                  temperature=0.6,\n",
    "                  top_p=0.9,\n",
    "                  top_k=50,\n",
    "                  repetition_penalty=1.2):\n",
    "    \"\"\"\n",
    "    It generates a response using the Llama2 7B HuggingFace Space as API\n",
    "    \"\"\"\n",
    "    response = LLAMA2_7B_CLIENT.predict(\n",
    "        prompt, system_prompt, max_new_tokens, temperature, top_p, top_k, repetition_penalty, api_name=\"/chat\" \n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def ask_mistral_7b(\n",
    "      prompt, \n",
    "      max_new_tokens=1024, \n",
    "      temperature=0.6,\n",
    "      top_p=0.9,\n",
    "      top_k=50,\n",
    "      repetition_penalty=1.2):\n",
    "    \"\"\"\n",
    "    It generates a response using the Mistral 7B HuggingFace unofficial space as API\n",
    "    \"\"\"\n",
    "    response = MISTRAL_7B_CLIENT.predict(\n",
    "        prompt, max_new_tokens, temperature, top_p, top_k, repetition_penalty, api_name=\"/chat\"\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def extract_json(gpt_answer):\n",
    "    text = gpt_answer.replace(\"\\\\n\", \"\\n\")\n",
    "    start_index = text.find(\"{\")\n",
    "    end_index = text.rfind(\"}\")\n",
    "\n",
    "    json_text = text\n",
    "    if start_index != -1 and end_index != -1 and start_index < end_index:\n",
    "        json_text = text[start_index : end_index + 1]\n",
    "\n",
    "    json_response = json_repair.loads(json_text)\n",
    "    return json_response    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbdda7-bbc2-46b5-8e28-935c03d7f98a",
   "metadata": {},
   "source": [
    "## Generated generic markers using GPT-4 \n",
    "\n",
    "These pro/cons markers are supposed to be generic as an indication of what GPT should look at to score the OCEAN traits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6be8a5-55e0-4dcb-84e8-32151386cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not finetuned. Only a sample\n",
    "markers = {\n",
    "    \"openness\": {\n",
    "        \"positive\": \"Searches and conversations about diverse topics, cultural interests, artistic activities, and philosophical discussions.\",\n",
    "        \"negative\": \"Limited variety in search topics, avoidance of abstract or theoretical discussions in conversations.\"\n",
    "    },\n",
    "    \"conscientiousness\": {\n",
    "        \"positive\": \"Searches related to planning, organization, and goal-setting. Conversations about personal achievements, detailed planning, and adherence to schedules.\",\n",
    "        \"negative\": \"Disorganized or last-minute search patterns, conversations indicating procrastination or disinterest in planning.\"\n",
    "    },\n",
    "    \"extraversion\": {\n",
    "        \"positive\": \"Online engagement with social events, searches about networking opportunities. Conversations that are energetic, involve many social topics, or planning social events.\",\n",
    "        \"negative\": \"Limited searches about social activities, conversations that are short, infrequent, or reveal a preference for solitude.\"\n",
    "    },\n",
    "    \"agreeableness\": {\n",
    "        \"positive\": \"Searches about volunteering, social causes, or how to help others. Conversations that are empathetic, understanding, and cooperative.\",\n",
    "        \"negative\": \"Searches indicating conflict, hostility, or self-centered interests. Conversations that are argumentative, lack empathy, or are overly competitive.\"\n",
    "    },\n",
    "    \"neuroticism\": {\n",
    "        \"positive\": \"Lack of excessive searches about worries, fears, or health anxieties. Balanced and calm nature in conversations.\",\n",
    "        \"negative\": \"Frequent searches about health anxieties, fears, or negative outcomes. Conversations that are often worried, stressed, or pessimistic.\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086941a-f82d-454f-a5dd-7a0d05ce11a4",
   "metadata": {},
   "source": [
    "## Handle data input in a way we can select date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f66ab-3cef-41f4-961e-14a605f45dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sortedcontainers import SortedList\n",
    "import os\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(self, date, messages):\n",
    "        self.date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        self.messages = messages\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.date < other.date\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Conversation(date='{self.date}', messages={self.messages})\"\n",
    "\n",
    "class SearchHistory:\n",
    "    def __init__(self, date, searches):\n",
    "        self.date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        self.searches = searches \n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.date < other.date\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SearchHistory(date='{self.date}', searches={self.searches})\"\n",
    "        \n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self):\n",
    "        self.conversations = SortedList()\n",
    "        self.search_history = SortedList()\n",
    "\n",
    "    def add_conversation(self, conversation):\n",
    "        self.conversations.add(conversation)\n",
    "\n",
    "    def add_search_history(self, search_history):\n",
    "        self.search_history.add(search_history) \n",
    "\n",
    "    def query_conversations(self, start_date, end_date):\n",
    "        start_datetime = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_datetime = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        return [conv for conv in self.conversations if start_datetime <= conv.date <= end_datetime]\n",
    "\n",
    "    def query_search_history(self, start_date, end_date):\n",
    "        start_datetime = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_datetime = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        return [hist for hist in self.search_history if start_datetime <= hist.date <= end_datetime]\n",
    "        \n",
    "\n",
    "def read_conversation_csv(file_path, as_str=False):\n",
    "    grouped_conversations = defaultdict(list)\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            grouped_conversations[row['date']].append({\n",
    "                'sender_name': row['sender_name'],\n",
    "                'content': row['content'],\n",
    "                'time': row['time']\n",
    "            })\n",
    "\n",
    "    return [Conversation(date, messages) for date, messages in grouped_conversations.items()]\n",
    "\n",
    "def read_search_history_csv(file_path, date):\n",
    "    grouped_search_histories = defaultdict(list)\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            grouped_search_histories[date].append({\n",
    "                'hour': row['hour'],\n",
    "                'title': row['title']\n",
    "            })\n",
    "\n",
    "    return [SearchHistory(date, searches) for date, searches in grouped_search_histories.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094a780-cedf-4943-9206-3aa8c53246b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a couple of files for testing\n",
    "data_manager = DataManager()\n",
    "\n",
    "conversation_data = read_conversation_csv('/home/betogaona7/almendra/Sensity/enclave-llm/assets/conversations_samples/NicolÃ² Magnante.csv')\n",
    "for conv in conversation_data:\n",
    "    data_manager.add_conversation(conv)\n",
    "\n",
    "search_history_data = read_search_history_csv(\"/home/betogaona7/almendra/Sensity/enclave-llm/assets/search_history_samples/2020-02/2020-02-07.csv\", \"2023-11-22\")\n",
    "for hist in search_history_data:\n",
    "    data_manager.add_search_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f5540-9d80-455d-8eb4-452373c014e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-02-22'\n",
    "end_date = '2023-12-31'\n",
    "conversations = data_manager.query_conversations(start_date, end_date)\n",
    "search_history = data_manager.query_search_history(start_date, end_date)\n",
    "\n",
    "print(f\"Conversations between {start_date} and {end_date}\")\n",
    "for day_conv in conversations:\n",
    "    print(day_conv)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Search history between {start_date} and {end_date}\")\n",
    "for day_search in search_history:\n",
    "    print(day_search)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b016b-28d8-40c4-90e8-475e2773c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(dir_path):\n",
    "    csv_paths = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_paths.append(os.path.join(root, file))\n",
    "    return csv_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe16c9-b1bf-47a9-a54d-7be6b7b09cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all data\n",
    "data_manager = DataManager()\n",
    "\n",
    "conversation_data_dir_path = \"/home/betogaona7/almendra/Sensity/enclave-llm/assets/conversations_samples\"\n",
    "search_history_data_dir_path = \"/home/betogaona7/almendra/Sensity/enclave-llm/assets/search_history_samples\"\n",
    "\n",
    "\n",
    "conversation_files =  get_file_paths(conversation_data_dir_path) \n",
    "search_history_files = get_file_paths(search_history_data_dir_path)\n",
    "\n",
    "for file_path in conversation_files:\n",
    "    file_data = read_conversation_csv(file_path)\n",
    "    for message in file_data:\n",
    "        data_manager.add_conversation(conv)\n",
    "\n",
    "for file_path in search_history_files:\n",
    "    search_date = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    file_data = read_search_history_csv(file_path, search_date)\n",
    "    for search in file_data:\n",
    "        data_manager.add_search_history(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb03e5-586a-433e-b78d-278c1b91a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-04-01'\n",
    "end_date = '2023-11-30'\n",
    "conversations = data_manager.query_conversations(start_date, end_date)\n",
    "search_history = data_manager.query_search_history(start_date, end_date)\n",
    "\n",
    "print(f\"Conversations between {start_date} and {end_date}: {len(conversations)}\")\n",
    "#for day_conv in conversations:\n",
    "    #print(day_conv)\n",
    "    #print(\"\\n\")\n",
    "\n",
    "print(f\"Search history between {start_date} and {end_date}: {len(search_history)}\")\n",
    "#for day_search in search_history:\n",
    "    #print(day_search)\n",
    "    #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385708b-10ff-4a27-b61a-6a00bd331cf8",
   "metadata": {},
   "source": [
    "## Prepare for chunking\n",
    "\n",
    "Since the items in the `conversations` and `search_history` lists are of the `Conversation` and `SearchHistory` types, respectively, we have the flexibility to construct the text strings we wish to use as data, selecting from the available fields as needed. (Also, make some indirect data cleaning, ex. not adding the date or time to each message/search content)\n",
    "\n",
    "However, some of these items can be too large, so we need to divide them into smaller segments. Since the goal for this notebook is only generate inputs and outputs for testing a workflow, I will select only those conversation and search history items that fall below a specified token threshold. Will define the chunking strategy later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4cd71-432d-4f7f-9476-bad22cd5a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Conversation items in a string with format sender_name: content\n",
    "\n",
    "conversations_str = []\n",
    "for conversation in conversations: \n",
    "    conv_str = \"\"\n",
    "    for message in conversation.messages:\n",
    "        conv_str += f\"{message['sender_name']}: {message['content']} \\n\"\n",
    "    conversations_str.append(conv_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3022f50-9ac4-417a-bca7-c835b0683dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversations_str[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87d2df-65d4-4dee-847b-5c00e51953f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert SearchHistory items in a string with format: For the date: date. The user history is: searches\n",
    "searches_str = []\n",
    "for search_history_item in search_history: \n",
    "    search_str = f\"For the date {search_history_item.date}. The user history is: \\n\"\n",
    "    for search in search_history_item.searches:\n",
    "        search_str += f\"{search['title']} \\n\"\n",
    "    searches_str.append(search_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db532b0-5dc7-4610-aec8-e029829d2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(searches_str[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad4e04-a7e2-4cc2-af58-68d4c187a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-16k')\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7dd6d-29fd-42b8-8cd5-d7a6a3e4cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_number_of_tokens(input):\n",
    "    # encoding for gpt-4, gpt-3.5\n",
    "    # functionality can be verified by copying the text here: https://platform.openai.com/tokenizer\n",
    "    tokens = tiktoken.get_encoding(\"cl100k_base\").encode(input)\n",
    "    return len(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017e5db-4878-40fd-b4f9-2d86152f262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = _get_number_of_tokens(searches_str[10])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e2ea5-5a02-4a56-835a-16a1c7e83857",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_threshold = 1024\n",
    "\n",
    "print(f\"before - conversations: {len(conversations_str)} searches: {len(searches_str)}\")\n",
    "\n",
    "filtered_conversations = [conv for conv in conversations_str if _get_number_of_tokens(conv) <= tokens_threshold]\n",
    "filtered_searches = [search for search in searches_str if _get_number_of_tokens(search) <= tokens_threshold]\n",
    "\n",
    "print(f\"after - conversations: {len(filtered_conversations)} searches: {len(filtered_searches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccfbba-42a6-4869-83fa-33b1f471eb2d",
   "metadata": {},
   "source": [
    "## Classify each conversation or search history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79dccc5-1606-4a6f-8509-dc01f20ff78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = {}\n",
    "for idx, chunk in enumerate(filtered_searches[:5]):\n",
    "    # Note that the prompt is not fine-tuned. Our aim is to simulate receiving an output for now, almost like mocking a unit test.\n",
    "    # this component will be update later\n",
    "    prompt = f\"\"\"\n",
    "    Please analyze the given search history and classify the associated OCEAN personality traits \\\n",
    "    (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) based on predefined markers. \n",
    "    \n",
    "    Assign each trait a level: high, medium, low, or none.\n",
    "    \n",
    "    Search History:\n",
    "    {chunk}\n",
    "    \n",
    "    Markers for OCEAN Traits:\n",
    "    {markers}\n",
    "    \n",
    "    Your task is to evaluate the search history against these markers and classify the level of each \\\n",
    "    OCEAN trait. \n",
    "    \n",
    "    Format your response as a JSON object, using the trait names as keys and the assigned levels as values. \n",
    "    \n",
    "    Expected JSON Output Format:\n",
    "    {{\n",
    "        \"openness\": \"high/medium/low/none\",\n",
    "        \"conscientiousness\": \"high/medium/low/none\",\n",
    "        \"extraversion\": \"high/medium/low/none\",\n",
    "        \"agreeableness\": \"high/medium/low/none\",\n",
    "        \"neuroticism\": \"high/medium/low/none\"\n",
    "    }}\n",
    "    \n",
    "    Note:\n",
    "    - Replace \"high/medium/low/none\" with the appropriate level based on your analysis.\n",
    "    - Ensure that the response strictly adheres to the JSON format specified.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ask_llama2_7b(prompt=prompt)\n",
    "        response = extract_json(response)\n",
    "        print(response)\n",
    "    except Exception as e: \n",
    "        print(f\"llama failed to respond: {e}\")\n",
    "         \n",
    "    labeled_data[idx] = response  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7e9f9-2ce3-40d6-96ef-7f80cb59f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d4e83-2fcc-46ac-bd0f-ed7da18f88a4",
   "metadata": {},
   "source": [
    "## Score according the labeled data \n",
    "\n",
    "We truncated the labeled data generation to include only five items, corresponding to the date range from 2023-04-01 to 2023-11-30.\n",
    "\n",
    "Our goal is to create a score dictionary for each OCEAN trait using the labeled data that contains at least one trait labeled as high and to save these results. Following this, we will replicate the process with a different date range, score the new set, and then update our initial scores to reflect the average score, continuing this process subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819ef38-45fb-4d8b-9813-660e6412ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(labeled_data):\n",
    "    chunks = []\n",
    "\n",
    "    for idx, traits in labeled_data.items():\n",
    "        for selected_trait in [\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]:\n",
    "            if traits[selected_trait] == \"high\":\n",
    "                chunks.append(filtered_searches[idx])\n",
    "                break\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40076cd0-1673-40b5-bdb1-ed0b2664c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_chunks = get_chunks(labeled_data)\n",
    "print(f\"chunks: {len(selected_chunks)}\")\n",
    "selected_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00c5dd-b204-4b14-96a9-7ea001c1cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b4e66-5edf-47a7-a5e8-bebed839a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD may need an additional chunk strategy here\n",
    "\n",
    "SCORE_TEMPLATE = \"\"\"\n",
    "Please assign a score between 0.0 and 1.0 to represent the level of each OCEAN trait in the following text. \n",
    "A score of 0 indicates the complete absence of the trait, while a score of 1 indicates the highest expression of the trait.\n",
    "\n",
    "text: {chunks}\n",
    "\n",
    "Output your answer as a single number without giving any explanation.\n",
    "\n",
    "Format your response as a JSON object, using the trait names as keys and the assigned levels as values. \n",
    "    \n",
    "    Expected JSON Output Format:\n",
    "    {{\n",
    "        \"openness\": \"float_number\",\n",
    "        \"conscientiousness\": \"float_number\",\n",
    "        \"extraversion\": \"float_number\",\n",
    "        \"agreeableness\": \"float_numer\",\n",
    "        \"neuroticism\": \"float_number\"\n",
    "    }}\n",
    "    \n",
    "Ensure that the response strictly adheres to the JSON format specified.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7b8a7-3f7b-4f4d-9c78-46650677322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_prompt = PromptTemplate(\n",
    "    input_variables=[\"chunks\"],\n",
    "    template=SCORE_TEMPLATE,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\"),\n",
    "    prompt=score_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e20ef-70df-4774-a989-6b1b3659cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.run({\"chunks\": selected_chunks})\n",
    "result = extract_json(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361dd88-3be4-4187-81c4-1490f26b24e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
