{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interests extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import httpx\n",
    "\n",
    "MAX_CONCURRENCY=x\n",
    "\n",
    "custom_client = AsyncOpenAI(\n",
    "  http_client=httpx.AsyncClient(\n",
    "    limits=httpx.Limits(\n",
    "      max_connections=MAX_CONCURRENCY,\n",
    "      max_keepalive_connections=MAX_CONCURRENCY\n",
    "    ),\n",
    "    timeout=60*10\n",
    "  ),\n",
    "  base_url=\"https://wao06rxq3acms1-8000.proxy.runpod.net/v1\"\n",
    ")\n",
    "\n",
    "no_match = 0\n",
    "errors = 0\n",
    "\n",
    "async def summarize_interests(data,prompt_1, prompt_2):\n",
    "  global custom_client\n",
    "\n",
    "  summarization_prompt = prompt_1\n",
    "  parsing_prompt = prompt_2\n",
    "\n",
    "  try:\n",
    "    answer1 = await custom_client.chat.completions.create(\n",
    "      model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": summarization_prompt+data},\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    answer2 = await custom_client.chat.completions.create(\n",
    "      model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": summarization_prompt+data}, \n",
    "        {\"role\": \"assistant\", \"content\": answer1.choices[0].message.content},\n",
    "        {\"role\": \"user\", \"content\": parsing_prompt},\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    raw = answer2.choices[0].message.content\n",
    "\n",
    "    # Use regex to extract the content between the square brackets\n",
    "    # We do this to get a python list from the string\n",
    "    match = re.search(r'\\[(.*?)\\]', raw)\n",
    "    if match:\n",
    "        # If a match is found, split the substring by comma\n",
    "        return match.group(1).replace(\"\\\"\", \"\").replace(\"'\",\"\").split(\",\")\n",
    "    else:\n",
    "        global no_match\n",
    "        no_match += 1\n",
    "        return []\n",
    "\n",
    "  except Exception as e:\n",
    "    global errors\n",
    "    errors += 1\n",
    "    return []\n",
    "  \n",
    "# From every day, we get maximum 35 records of raw data to call the LLM with\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "\n",
    "chunk_size = 35\n",
    "\n",
    "interests = defaultdict(list)\n",
    "tasks_dict = defaultdict(list)\n",
    "\n",
    "for filename in get_filenames():\n",
    "    df = pd.read_csv(filename)\n",
    "    date = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    inputs = df[\"title\"].tolist()\n",
    "\n",
    "    for i in range(0, len(inputs), chunk_size):\n",
    "        # TODO: change this code to use the two prompt sets form the Google doc\n",
    "        tasks_dict[date].append(summarize_interests(\"\\n\".join(inputs[i:i+chunk_size])))\n",
    "\n",
    "\n",
    "# Code to run the tasks in parallel, while keeping the date information for each task\n",
    "\n",
    "from asyncio import Semaphore\n",
    "\n",
    "wrapped_tasks = []\n",
    "\n",
    "async def wrap_task_with_date(sem, date, t):\n",
    "    async with sem:\n",
    "        result = await t\n",
    "        return (date, result)\n",
    "\n",
    "sem = Semaphore(MAX_CONCURRENCY)\n",
    "for date, tasks in tasks_dict.items():\n",
    "    wrapped_tasks.extend([wrap_task_with_date(sem, date, task) for task in tasks])\n",
    "\n",
    "import json\n",
    "\n",
    "results_dict = defaultdict(list)\n",
    "results = await tqdm_asyncio.gather(*wrapped_tasks, smoothing=0)\n",
    "    \n",
    "for date, result in results:\n",
    "    results_dict[date].extend(result)\n",
    "    json.dump(results_dict[date], open(f\"../_data/interests/{date}.json\", \"w\"))\n",
    "\n",
    "# TODO: separate the results form prompt set A and B\n",
    "\n",
    "df[\"interests_a\"] = interests_a\n",
    "df[\"interests_b\"] = interests_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\")\n",
    "embeddings = []\n",
    "\n",
    "# TODO: run this separately for the results of each of the two prompt sets\n",
    "\n",
    "for interest in df[\"interests_a\"]:\n",
    "    embeddings.append(model.encode(interest))\n",
    "\n",
    "df[\"embeddings_a\"] = embeddings\n",
    "df[\"embeddings_b\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import pairwise_distances\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "import cupy as cp  \n",
    "import cuml\n",
    "\n",
    "embeddings_gpu = cp.asarray(df[\"embeddings_a\"].to_list())\n",
    "\n",
    "umap_model = cuml.UMAP(n_neighbors=15,\n",
    "                       n_components=100, \n",
    "                       min_dist=0.1, \n",
    "                       metric='cosine')\n",
    "reduced_data_gpu = umap_model.fit_transform(embeddings_gpu)\n",
    "\n",
    "cosine_dist = pairwise_distances(reduced_data_gpu, metric='cosine')\n",
    "\n",
    "clusterer = HDBSCAN(min_cluster_size=5, \n",
    "                    gen_min_span_tree=True,\n",
    "                    metric=\"precomputed\",\n",
    "                    cluster_selection_epsilon=0.02) \n",
    "cluster_labels = clusterer.fit_predict(cosine_dist.astype(np.float64).get())\n",
    "\n",
    "df[\"clusters_a\"] = cluster_labels\n",
    "df[\"clusters_b\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
