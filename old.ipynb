{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  session extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# function to turn string returned from the LLM into valid python dictionary\n",
    "def extract_json(text):\n",
    "    def find_closing(text, open_pos, open_char, close_char):\n",
    "        balance = 0\n",
    "        for i in range(open_pos, len(text)):\n",
    "            if text[i] == open_char:\n",
    "                balance += 1\n",
    "            elif text[i] == close_char:\n",
    "                balance -= 1\n",
    "                if balance == 0:\n",
    "                    return i\n",
    "        return -1\n",
    "\n",
    "    obj_start = text.find(\"{\")\n",
    "    arr_start = text.find(\"[\")\n",
    "\n",
    "    if obj_start == -1 and arr_start == -1:\n",
    "        return {}, None  \n",
    "\n",
    "    start_index = obj_start if arr_start == -1 or (obj_start != -1 and obj_start < arr_start) else arr_start\n",
    "    open_char = \"{\" if start_index == obj_start else \"[\"\n",
    "    close_char = \"}\" if open_char == \"{\" else \"]\"\n",
    "\n",
    "    end_index = find_closing(text, start_index, open_char, close_char)\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        json_text = text[start_index:end_index + 1]\n",
    "        try:\n",
    "            json_response = json.loads(json_text)\n",
    "            return json_response, text[end_index + 1:]\n",
    "        except json.JSONDecodeError:\n",
    "            return {}, None  \n",
    "    else:\n",
    "        return {}, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"\"\"\n",
    "    Analyze the provided list of Google search records to identify distinct topic groups. For each group, create a summary in the JSON format below. Ensure each summary includes: \n",
    "\n",
    "    - `time_start`: The start time of the first search in the group.\n",
    "    - `time_end`: The end time of the last search in the group.\n",
    "    - `description`: A detailed account of the searches and site visits, enriched with inferred user intent and additional insights into the topic.\n",
    "\n",
    "    Each `description` should not only recap the searches but also offer a deeper understanding of what the user might be seeking or the broader context of their inquiries. Group searches based on thematic relevance and timing. \n",
    "\n",
    "    Example of JSON output format:\n",
    "\n",
    "    {\n",
    "    \"time_start\": \"HH:MM\",\n",
    "    \"time_end\": \"HH:MM\",\n",
    "    \"description\": \"Elaborate on what the user did and why, based on the search terms and visited pages.\",\n",
    "    }\n",
    "    \n",
    "    Here is a list of searches:\n",
    "\"\"\"\n",
    "chunk_size = 15\n",
    "\n",
    "\n",
    "for filename in tqdm(get_filenames()):\n",
    "\n",
    "    # We send chunks of 15 raw search records to the LLM to get more accurate results\n",
    "    for i in tqdm(range(0, len(df), chunk_size)):\n",
    "        chunk = df.iloc[i : i + chunk_size]\n",
    "\n",
    "        answer = get_completion(f\"{summary_prompt}\\n{chunk}\")\n",
    "\n",
    "        # Sometimes the LLM returns multipe json objects in a list\n",
    "        # Some other times it returns a single json object\n",
    "        # We need to handle both cases\n",
    "        parsed_results = []\n",
    "        while answer:\n",
    "            parsed_result, answer = extract_json(answer)\n",
    "\n",
    "            if parsed_result:\n",
    "                if isinstance(parsed_result, dict):\n",
    "                    parsed_results.append(parsed_result)\n",
    "                elif isinstance(parsed_result, list):\n",
    "                    parsed_results.extend(parsed_result)\n",
    "\n",
    "        parsed_df = pd.concat(\n",
    "            [parsed_df, pd.DataFrame(parsed_results)], ignore_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "    parsed_df.to_csv(os.path.join(out_path, f\"{date}.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\")\n",
    "embeddings = model.encode(parsed_df[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import os\n",
    "\n",
    "conn = psycopg.connect(**psycopg.conninfo.conninfo_to_dict(os.environ[\"DATABASE_URL\"]))\n",
    "conn.autocommit = True\n",
    "\n",
    "conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "register_vector(conn)\n",
    "\n",
    "conn.execute(\"DROP TABLE IF EXISTS documents\")\n",
    "conn.execute(\n",
    "    \"\"\"CREATE TABLE documents (\n",
    "             id bigserial PRIMARY KEY, \n",
    "             description text,\n",
    "             date DATE, \n",
    "             time TIME,\n",
    "             raw text,\n",
    "             embedding vector(1536) \n",
    "    )\"\"\"\n",
    ")\n",
    "\n",
    "# TODO populate the documents table with the dataframe contents\n",
    "\n",
    "conn.execute(\"DROP TABLE IF EXISTS edges\")\n",
    "conn.execute(\n",
    "    \"\"\"CREATE TABLE edges (\n",
    "             id bigserial PRIMARY KEY, \n",
    "             parent_id bigint,\n",
    "             child_id bigint,\n",
    "             weight float\n",
    "    )\"\"\"\n",
    ")\n",
    "\n",
    "# Code to merge sessions that are the same (this can happen because of the chunking we do in step 1)\n",
    "# for example: session 1 starts at 10:15 and ends at 10:30, session 2 starts at 10:30 and ends at 10:45 and they have the same description\n",
    "\n",
    "# Calculate the 10th percentile for time intervals (in seconds)\n",
    "time_threshold = conn.execute(\"\"\"\n",
    "WITH LaggedDocuments AS (\n",
    "    SELECT\n",
    "        date,\n",
    "        time,\n",
    "        LAG(time) OVER (ORDER BY date, time) AS prev_time\n",
    "    FROM\n",
    "        documents\n",
    "    WHERE\n",
    "        is_taxonomy = FALSE\n",
    "),\n",
    "TimeDifferences AS (\n",
    "    SELECT\n",
    "        EXTRACT(EPOCH FROM (time - prev_time)) AS time_diff\n",
    "    FROM\n",
    "        LaggedDocuments\n",
    "    WHERE\n",
    "        time > prev_time\n",
    ")\n",
    "SELECT\n",
    "    percentile_cont(0.10) WITHIN GROUP (ORDER BY time_diff) AS time_interval_10th\n",
    "FROM\n",
    "    TimeDifferences;\n",
    "\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "# Calculate the 90th percentile for embedding similarities using cosine similarity\n",
    "embedding_similarity_threshold = conn.execute(\"\"\"\n",
    "    WITH CosineSimilarities AS (\n",
    "        SELECT\n",
    "            date,\n",
    "            time,\n",
    "            1 - (embedding <=> LAG(embedding) OVER (ORDER BY date, time)) AS cosine_similarity\n",
    "        FROM\n",
    "            documents\n",
    "        WHERE\n",
    "            is_taxonomy = FALSE\n",
    "    ),\n",
    "    FilteredSimilarities AS (\n",
    "        SELECT\n",
    "            cosine_similarity\n",
    "        FROM\n",
    "            CosineSimilarities\n",
    "        WHERE\n",
    "            cosine_similarity IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        percentile_cont(0.90) WITHIN GROUP (ORDER BY cosine_similarity) AS embedding_similarity_90th\n",
    "    FROM\n",
    "        FilteredSimilarities;\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(f\"Time threshold: {time_threshold} seconds. Embedding similarity threshold: {embedding_similarity_threshold}\")\n",
    "\n",
    "# Time threshold: 60.0 seconds. Embedding similarity threshold: 0.8296323921172489\n",
    "# this means that we will merge documents that are within 60 seconds of each other and have an embedding similarity of 0.83 or higher\n",
    "\n",
    "# merge similar documents within the time and embedding similarity thresholds\n",
    "records = conn.execute(\n",
    "    \"\"\"\n",
    "SELECT a.id, b.id, (1 - (a.embedding <=> b.embedding)) AS similarity\n",
    "FROM documents a\n",
    "JOIN documents b ON a.id < b.id\n",
    "WHERE ABS(EXTRACT(EPOCH FROM (\n",
    "    (a.date || ' ' || a.time)::timestamp - \n",
    "    (b.date || ' ' || b.time)::timestamp))/60) <= %s\n",
    "    AND NOT (a.is_taxonomy = TRUE OR b.is_taxonomy = TRUE )\n",
    "\"\"\",\n",
    "    ((time_threshold / 60),),\n",
    ").fetchall()\n",
    "\n",
    "\n",
    "candidates_to_merge = []\n",
    "for record in records:\n",
    "    doc_id_a, doc_id_b, similarity = record\n",
    "    if similarity >= embedding_similarity_threshold:\n",
    "        candidates_to_merge.append((doc_id_a, doc_id_b))\n",
    "\n",
    "# function to create the dag \n",
    "# for every document, we find the most similar document that cmae before in time and over a certain similarity threshold (0.6)\n",
    "# we create an edge between the two documents with a weight of 1 - similarity (the distance)\n",
    "\n",
    "def create_dag():\n",
    "    return conn.execute(\n",
    "        \"\"\"\n",
    "    WITH DocumentPairs AS (\n",
    "        SELECT\n",
    "            a.id AS doc_id,\n",
    "            b.id AS compared_doc_id,\n",
    "            b.is_taxonomy AS compared_is_taxonomy,\n",
    "            (1 - (a.embedding <=> b.embedding)) AS similarity,\n",
    "            a.date AS doc_date,\n",
    "            a.time AS doc_time,\n",
    "            b.date AS compared_doc_date,\n",
    "            b.time AS compared_doc_time\n",
    "        FROM\n",
    "            documents a\n",
    "        JOIN\n",
    "            documents b ON a.id != b.id AND \n",
    "                        (a.date > b.date OR (a.date = b.date AND a.time > b.time))\n",
    "    ),\n",
    "    RankedPairs AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER(PARTITION BY doc_id, compared_is_taxonomy ORDER BY similarity DESC) AS rank\n",
    "        FROM\n",
    "            DocumentPairs\n",
    "    ), FilteredPairs1 as (\n",
    "        SELECT\n",
    "            doc_id,\n",
    "            compared_doc_id,\n",
    "            compared_is_taxonomy,\n",
    "            similarity\n",
    "        FROM\n",
    "            RankedPairs\n",
    "        WHERE\n",
    "            rank = 1 AND\n",
    "            ((compared_is_taxonomy = FALSE AND similarity > 0.6) OR compared_is_taxonomy = TRUE)\n",
    "    ), FilteredPairs2 as (\n",
    "        SELECT\n",
    "            doc_id,\n",
    "            MAX(compared_doc_id) AS compared_doc_id,\n",
    "            MAX(similarity) AS similarity\n",
    "        FROM\n",
    "            FilteredPairs1\n",
    "        group by doc_id\n",
    "    )\n",
    "    INSERT INTO edges (parent_id, child_id, weight)\n",
    "    SELECT\n",
    "        doc_id,\n",
    "        compared_doc_id,\n",
    "        1-similarity\n",
    "    FROM\n",
    "        FilteredPairs2;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# send results to GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
