{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import os\n",
    "\n",
    "conn = psycopg.connect(**psycopg.conninfo.conninfo_to_dict(os.environ[\"DATABASE_URL\"]))\n",
    "conn.autocommit = True\n",
    "\n",
    "conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from datetime import datetime\n",
    "\n",
    "min_datetime = conn.execute(\n",
    "    \"SELECT MIN(CAST(date || ' ' || time AS TIMESTAMP)) FROM documents WHERE NOT is_taxonomy\"\n",
    ").fetchone()[0]\n",
    "max_datetime = conn.execute(\n",
    "    \"SELECT MAX(CAST(date || ' ' || time AS TIMESTAMP)) FROM documents WHERE NOT is_taxonomy\"\n",
    ").fetchone()[0]\n",
    "\n",
    "def get_node_color(date, time_end):\n",
    "    if time_end == 'None':\n",
    "        time_end = \"00:00:00\"   \n",
    "    # convert the dates to datetime objects\n",
    "    date = datetime.strptime(f\"{date} {time_end}\", \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "\n",
    "\n",
    "    cmap = plt.get_cmap('coolwarm')  # Choose a colormap\n",
    "    norm = mcolors.Normalize(vmin=min_datetime.timestamp(), vmax=max_datetime.timestamp())  # Normalize the dates\n",
    "\n",
    "    # Convert the date to a float value between 0 and 1\n",
    "    date_value = norm(date)\n",
    "\n",
    "    # Get the corresponding color from the colormap\n",
    "    color = cmap(date_value)\n",
    "\n",
    "    # Convert the color to a hex value\n",
    "    return mcolors.to_hex(color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new directed graph\n",
    "dag = nx.DiGraph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for row in conn.execute(\n",
    "    \"SELECT id, description, date, time, is_taxonomy, raw FROM documents\"\n",
    "):\n",
    "    if row[4]:\n",
    "        pass\n",
    "        # dag.add_node(row[0], label=str(row[5]), title=row[1], color=\"#00FF00\")\n",
    "    else:\n",
    "        dag.add_node(\n",
    "            row[0],\n",
    "            label=str(row[5]),\n",
    "            title=row[1],\n",
    "            date=str(row[2])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges to the graph\n",
    "for row in conn.execute(\n",
    "    \"\"\"SELECT parent_id, child_id, weight FROM edges WHERE child_id not in (\n",
    "        select id from documents where is_taxonomy = TRUE\n",
    "    )\"\"\"\n",
    "):  # WHERE parent_id IS NOT NULL\n",
    "    parent_id, child_id, weight = row\n",
    "    dag.add_edge(parent_id, child_id, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5 connected components\n",
    "components = nx.weakly_connected_components(dag)\n",
    "subgraphs = [(dag.subgraph(c), len(c)) for c in components]\n",
    "subgraphs_sorted_by_size = sorted(subgraphs, key=lambda x: x[1], reverse=True)\n",
    "largest_5_subgraphs = subgraphs_sorted_by_size[:10]\n",
    "largest_5_graphs = [g[0] for g in largest_5_subgraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "summaries = []\n",
    "\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "prompt = \"\"\"\n",
    "I will provide you a list of Google searches I have performed through time to pursue a certain goal / skill / knowledge.\n",
    "What can you tell of my journey?\n",
    "\"\"\"\n",
    "\n",
    "def get_completion(records):\n",
    "    res = client.chat.completions.create(\n",
    "      model=\"gpt-4\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\\n{records}\"},\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    return res.choices[0].message.content\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2378e89f8fe4ed099fe7fdc6636a4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use openai gpt to generate a summary for each subgraph\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "completions = []\n",
    "for graph in tqdm(largest_5_graphs):\n",
    "    # longest_path = nx.dag_longest_path(graph)\n",
    "    # graph_nodes = [graph.nodes(data=True)[node] for node in longest_path]\n",
    "    # graph_nodes = sorted(graph_nodes, key=lambda x: x['date'])\n",
    "    \n",
    "    records = \"\\n\".join([f\"{node['date']} {node['label']}\" for node in graph_nodes])\n",
    "    \n",
    "    res = get_completion(records)\n",
    "    completions.append([records, res])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
