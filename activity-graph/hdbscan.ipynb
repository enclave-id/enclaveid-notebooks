{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here first we reduce dims, cluster and then we build the trees with the full 1.5k dims to speed up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import os\n",
    "\n",
    "conn = psycopg.connect(**psycopg.conninfo.conninfo_to_dict(os.environ[\"DATABASE_URL\"]))\n",
    "conn.autocommit = True\n",
    "\n",
    "conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = conn.execute(\n",
    "    \"\"\"\n",
    "    SELECT id, description, date, time, is_taxonomy, raw, embedding FROM documents \n",
    "    WHERE is_taxonomy = FALSE AND date > '2023-07-01'\n",
    "    \"\"\"\n",
    ").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<psycopg.Cursor [COMMAND_OK] [IDLE] (host=localhost port=5433 database=enclaveid) at 0x7fac3e365fd0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(\"DROP TABLE IF EXISTS edges\")\n",
    "conn.execute(\n",
    "    \"\"\"CREATE TABLE edges (\n",
    "             id bigserial PRIMARY KEY, \n",
    "             parent_id bigint,\n",
    "             child_id bigint,\n",
    "             weight float\n",
    "    )\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "raw = []\n",
    "date = []\n",
    "ids = []\n",
    "for row in documents:\n",
    "    embeddings.append(row[6])\n",
    "    raw.append(row[5])\n",
    "    date.append(row[2]) \n",
    "    ids.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=15,\n",
    "                       n_components=100, \n",
    "                       min_dist=0.1, \n",
    "                       metric='cosine') \n",
    "reduced_data = umap_model.fit_transform(embeddings)\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, # minimum size of clusters\n",
    "                            gen_min_span_tree=True) # useful for visualization\n",
    "cluster_labels = clusterer.fit_predict(reduced_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for label, raw_item, date,id in zip(cluster_labels, raw, date, ids):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = [(raw_item, date, id)]\n",
    "    else:\n",
    "        clusters[label].append((raw_item, date, id))\n",
    "\n",
    "clustered_raw_items = [np.array(clusters[label]) for label in clusters if label != -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clusters = []\n",
    "for cluster in clustered_raw_items:\n",
    "    sorted_clusters.append(sorted(cluster, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 largest clusters\n",
    "sorted_clusters = sorted(sorted_clusters, key=lambda x: len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(cluster):\n",
    "    return conn.execute(\n",
    "        \"\"\"\n",
    "    WITH Documents AS (\n",
    "        SELECT\n",
    "            id,\n",
    "            description,\n",
    "            date,\n",
    "            time,\n",
    "            is_taxonomy,\n",
    "            raw,\n",
    "            embedding\n",
    "        FROM\n",
    "            documents\n",
    "        WHERE\n",
    "            id = ANY(%(ids)s)\n",
    "    ),\n",
    "    DocumentPairs AS (\n",
    "        SELECT\n",
    "            ranked_similarities.doc_id,\n",
    "            ranked_similarities.compared_doc_id,\n",
    "            ranked_similarities.similarity\n",
    "        FROM (\n",
    "            SELECT\n",
    "                a.id AS doc_id,\n",
    "                b.id AS compared_doc_id,\n",
    "                (1 - (a.embedding <=> b.embedding)) AS similarity,\n",
    "                ROW_NUMBER() OVER (PARTITION BY a.id ORDER BY (1 - (a.embedding <=> b.embedding)) DESC) AS rank\n",
    "            FROM\n",
    "                Documents a\n",
    "            JOIN\n",
    "                Documents b ON a.id != b.id \n",
    "                AND (a.date > b.date OR (a.date = b.date AND a.time > b.time))\n",
    "        ) AS ranked_similarities\n",
    "        WHERE\n",
    "            ranked_similarities.rank = 1\n",
    "    )\n",
    "    INSERT INTO edges (parent_id, child_id, weight)\n",
    "    SELECT\n",
    "        doc_id,\n",
    "        compared_doc_id,\n",
    "        1-similarity\n",
    "    FROM\n",
    "        DocumentPairs;\n",
    "\"\"\", {\"ids\": [item[2] for item in cluster]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new directed graph\n",
    "dag = nx.DiGraph()\n",
    "\n",
    "for cluster in sorted_clusters[:10]:\n",
    "    for item in cluster:\n",
    "        dag.add_node(item[2], label=item[0])\n",
    "\n",
    "    create_edges(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges to the graph\n",
    "for row in conn.execute(\"SELECT parent_id, child_id, weight FROM edges\"): \n",
    "    parent_id, child_id, weight = row\n",
    "    dag.add_edge(parent_id, child_id, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(dag, \"../_data/dag_hdbscan.graphml\") # for graphia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
